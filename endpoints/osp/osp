#!/usr/bin/env bash
# -*- mode: sh; indent-tabs-mode: nil; sh-basic-offset: 4 -*-
# vim: autoindent tabstop=4 shiftwidth=4 expandtab softtabstop=4 filetype=bash

# This script implements the 'osp' endpoint for rickshaw.  It runs 1 or more
# clients and servers as instances/VMs in an Openstack cluster.
#
# Usage:
# osp [--validate] --endpoint-opts=host=<host>,user=<user>,client:n-m,o-p,server:n-m,o-p 
#                    --run-id <id> --base-run-dir --image <location>
#                    --roadblock-server <host> --roadblock-id <id> --roadblock-passwd=<passwd>
#
# If --validate is used all options after client/server will be ignored
#

# Source the base file for common functions and config
this_endpoint_dir=$(dirname `readlink -e $0` | sed -e 'sX/binXX')
endpoint_base_dir=$(cd $this_endpoint_dir >/dev/null && cd .. && /bin/pwd)
if [ -e "$endpoint_base_dir/base" ]; then
    . "$endpoint_base_dir/base"
else
    echo "Could not find endpoint source file "$endpoint_base_dir/base", exiting"
    exit 1
    exit
fi
chroot_rbind_mounts="proc dev sys lib/modules usr/src boot var/run"
image_cache_size=3
default_flavor="nfv-large"
default_availability_zone="nova"
osruntime="chroot"
host_mounts=""
endpoint_name="osp"
instance_prefix="rickshaw"
project_name="crucible-rickshaw"
controllers_tool_collect="0"
computes_tool_collect="0"
declare -A cpuPartitioning
declare -A flavor
declare -A availability_zone
declare -A ips

function cleanup_json() {
    local json="$1"; shift

    echo "cleanup_json: processing ${json}"
    mv ${json} ${json}.tmp
    if jq . ${json}.tmp > ${json}; then
        rm ${json}.tmp
    else
        abort_error "cleanup_json failed to process ${json}" endpoint-deploy-begin
    fi
}

function endpoint_osp_test_stop() {
    local msgs_dir="$1"; shift
    local test_id="$1"; shift
    echo "Running endpoint_osp_test_stop"

    # Delete network related openstack items here (router, floating ip, firewall?)
}

function endpoint_osp_test_start() {
    # This function runs right after a server starts any service and right before a client starts
    # and tries to contect the server's service.  The purpose of this function is to do any
    # work which ensures the client can contact the server.  In some cases there may be nothing
    # to do.  Regardless of the work, the endpoint needs to relay what IP & ports the client
    # needs to use in order to reach the server.  In some cases that may be the information the
    # server has provided to the endpoint, or this information has changed because the endpoint
    # created some sort of proxy to reach the server.

    local msgs_dir="$1"; shift
    local test_id="$1"; shift
    local tx_msgs_dir="$1"; shift
    echo "Running endpoint_osp_test_start"
    # Creating any service or ingress only works if any servers provided information about its
    # IP and ports.
    local this_msg_file="${msgs_dir}/${test_id}:server-start-end.json"
    if [ -e $this_msg_file ]; then
        echo "Found $this_msg_file"
        # Extract the cs-label (server-n, client-y) the IP, and the ports this benchmark is using:
        # server-1 1.2.3.4 30002 30003
        cat $this_msg_file | jq -r '.received[] | if .payload.message.command == "user-object" and .payload.message."user-object".svc.ports then [ .payload.sender.id, .payload.message."user-object".svc.ip, .payload.message."user-object".svc.ports ] | flatten | tostring   else null end' | grep -v null | sed -e 's/"//g' -e 's/\[//' -e 's/\]//' -e 's/,/ /g' >"$endpoint_run_dir/ports.txt"

        while read -u 9 line; do
            echo "line: $line"
            # TODO: set up OSP network services here, like firewall, router, etc
        done 9< "$endpoint_run_dir/ports.txt"
    else
        echo "Did not find $this_msg_file"
    fi
}

function osp_req_check() {
    if [ -z "$host" ]; then
        exit_error "osp host is not defined"
    fi
    verify_ssh_login $user $host
    # TODO: source overcloudrc or equivalent init file to do osp stuff.  Error out if not found
    osp_cmd=". ./overcloudrc && openstack server list"
    do_ssh ${user}@${host} "$osp_cmd 2>&1" 2>&1 >msg
    rc=$?
    if [ $rc -gt 0 ]; then
        exit_error "Could not run 'openstack' on overcloud host: $osp_cmd"
    fi
    # Validation returns what clients and servers would be used and the userenv
    if [ "$do_validate" == 1 ]; then
        echo_clients_servers
        echo "userenv $userenv"
        exit
    fi
}

function get_opt_field() {
    local input=${1}
    local field=${2}

    case "${field}" in
        "key")
            field="1"
            ;;
        "value")
            field="2-" # return all fields from 2 on, allowing for the delimiter to be a part of the value
            ;;
    esac

    echo "${input}" | cut -d ':' -f ${field}
}

function process_osp_opts() {
    local endpoint_opts="$1"
    for opt in `echo $endpoint_opts | sed -e 's/,/ /g'`; do
        arg=`echo $opt| awk -F: '{print $1}'`
        # The $val may have : in it, so don't use awk to get only the second field
        val=`echo $opt | sed -e s/^$arg://`
        case "$arg" in
            ospconfig)
                ospconfig=$val
                ;;
            client|server|clients|servers)
                addto_clients_servers "$arg" "$val"
                ;;
            host)
                host=$val
                if [ -z "$controller_ipaddr" ]; then
                    controller_ipaddr=`get_controller_ip $host`
                fi
                ;;
            controller-ip)
                controller_ipaddr=$val
                ;;
            user)
                user=$val
                ;;
            userenv)
                userenv=$val
                ;;
            controllers-tool-collect)
                controllers_tool_collect=$val
                ;;
            computes-tool-collect)
                computes_tool_collect=$val
                ;;
            cpu-partitioning)
                # cpu-partitioning is per engine:
                # option format::  cpu-partitioning:<engine-label>:<value>
                # <engine-label> can be 'default' to apply to any engine that is not explicitly specified
                #TODO: validate correct format of <engine-label>
                name=$(get_opt_field "${val}" "key")
                value=$(get_opt_field "${val}" "value")
                if [ ! -z "$name" -a ! -z "$value" ]; then
                    cpuPartitioning[$name]=$value
                else
                    exit_error "Could not properly decode cpu-partitioning for '$val'"
                fi
                ;;
            osruntime)
                osruntime=$val
                ;;
            flavor)
                flavor="$val"
                # flavor is per engine:
                # option format::  flavor:<engine-label>:<flavor>
                # <engine-label> can be 'default' to apply to any engine that is not explicitly specified
                this_key=$(get_opt_field "${val}" "key")
                this_val=$(get_opt_field "${val}" "value")
                flavor[$this_key]=$this_val
                ;;
            availability-zone)
                # node (compute node hostname) is per engine:
                # option format::  node:<engine-label>:<node>
                # <engine-label> can be 'default' to apply to any engine that is not explicitly specified
                this_key=$(get_opt_field "${val}" "key")
                this_value=$(get_opt_field "${val}" "value")
                availability_zone[$this_key]=$this_value
                ;;
            *)
                if echo $arg | grep -q -- "="; then
                    echo "You can't use a \"=\" for assignment in endpoint options"
                    echo "You must use \":\", like `echo $arg | sed -e 's/=/:/'`"
                fi
                exit_error "osp endpoint option [$arg] not supported"
                ;;
        esac
    done

    if [ "${unique_project}" == "1" ]; then
        project_name+="--${run_id}-${endpoint_label}"
    fi
}

function create_remotehost_engines() {
    echo "create_remotehost_engines()"
}

function create_servers() {
    typeset -n ref1=$1; shift # caller-provided variable name (call-by-reference)
    ref1=""
    local type="$1"; shift
    local instances=""
    echo "vm_ssh_key_name: [$vm_ssh_key_name]"
    echo "vm_ssh_key_file: [$vm_ssh_key_file]"
    do_ssh ${user}@${host} "mkdir -p $vm_ssh_key_dir"
    echo "checking for existence of ssh key"
    do_ssh ${user}@${host} "$osp_pre_cmd openstack keypair list --quote minimal -f csv | grep -v Name,Fingerprint" >${endpoint_run_dir}/openstack-keys.txt
    existing_key=`cat ${endpoint_run_dir}/openstack-keys.txt | awk -F, '{print $1}' | grep -v \"Name\" | grep $vm_ssh_key_name` 
    if [ "[$existing_key]" == "[$vm_ssh_key_name]" ]; then
        echo "deleting existing ssh key [$vm_ssh_key_name]"
        do_ssh ${user}@${host} "$osp_pre_cmd openstack keypair delete $vm_ssh_key_name" 
        echo "remaining keypairs"
        do_ssh ${user}@${host} "$osp_pre_cmd openstack keypair list --quote minimal -f csv"
    fi
    echo "creating new ssh key:"
    echo "[$osp_pre_cmd openstack keypair create $vm_ssh_key_name >$vm_ssh_key_file && chmod 600 $vm_ssh_key_file]"
    do_ssh ${user}@${host} "$osp_pre_cmd openstack keypair create $vm_ssh_key_name >$vm_ssh_key_file && chmod 600 $vm_ssh_key_file"
    local osp_servers=""

    while [ $# -gt 0 ]; do
        cs_label=$1; shift
        vm_name="rickshaw-$run_id-$cs_label"
        mgmt_port_name="mgmt-$vm_name"

        this_flavor=$default_flavor
        if [ ! -z "${flavor[$cs_label]}" ]; then
            this_flavor=${flavor[$cs_label]}
        elif [ ! -z "${flavor[default]}" ]; then
            this_flavor=${flavor[default]}
        fi

        this_availability_zone=$default_availability_zone
        if [ ! -z "${availability_zone[$cs_label]}" ]; then
            this_availability_zone=${availability_zone[$cs_label]}
        elif [ ! -z "${availability_zone[default]}" ]; then
            this_availability_zone=${availability_zone[default]}
        fi

        echo "checking for existing server"
        do_ssh ${user}@${host} "$osp_pre_cmd openstack server list --quote minimal -f csv" >${endpoint_run_dir}/openstack-existing-servers.txt
        echo "existing servers:"
        cat ${endpoint_run_dir}/openstack-existing-servers.txt
        existing_server=`cat ${endpoint_run_dir}/openstack-existing-servers.txt | grep -v ID,Name,Status,Networks,Image,Flavor | grep $vm_name | awk -F, '{print $2}'`
        echo "existing_server: [$existing_server]"
        if [ "$existing_server" == "$vm_name" ]; then
            echo "deleting existing server [$vm_name]"
            do_ssh ${user}@${host} "$osp_pre_cmd openstack server delete $vm_name --wait" 
        fi

        echo "checking for existing port"
        do_ssh ${user}@${host} "$osp_pre_cmd openstack port list -f csv --quote minimal | grep -v ID,Name,MAC" >${endpoint_run_dir}/openstack-existing-ports.txt
        echo "existing ports:"
        cat ${endpoint_run_dir}/openstack-existing-ports.txt
        echo "existing_port: [$existing_port]"
        existing_port=`cat ${endpoint_run_dir}/openstack-existing-ports.txt | grep -v ID,Name,Mac | grep $mgmt_port_name | awk -F, '{print $2}'`
        if [ "$existing_port" == "$mgmt_port_name" ]; then
            echo "deleting existing port [$mgmt_port_name]"
            do_ssh ${user}@${host} "$osp_pre_cmd openstack port delete $mgmt_port_name" 
        fi
        do_ssh ${user}@${host} "$osp_pre_cmd openstack port create --network management --no-security-group --disable-port-security $mgmt_port_name"

        echo "creating openstack server $vm_name"
        do_ssh ${user}@${host} "$osp_pre_cmd openstack server create --flavor nfv-large --nic port-id=$mgmt_port_name --key-name $vm_ssh_key_name --image stream9 --availability-zone nova  $vm_name --wait" >${endpoint_run_dir}/openstack-create-$vm_name.txt
        echo "getting openstack port information for $vm_name"
        do_ssh ${user}@${host} "$osp_pre_cmd openstack port show $mgmt_port_name -f json" >${endpoint_run_dir}/openstack-port-$mgmt_port_name.json
        echo "finding server IP"
        this_server_ip=`jq -r .fixed_ips[0].ip_address ${endpoint_run_dir}/openstack-port-$mgmt_port_name.json`
        echo "this_server_ip: [$this_server_ip]"
        do_ssh ${user}@${host} "$osp_pre_cmd ssh-keygen -R $this_server_ip"
        osp_servers+=" $cs_label"
        ips[$cs_label]=$this_server_ip
    done
    ref1=$osp_servers
    sleep 90
}

function launch_osruntime() {
    local this_cs_label this_cs_log_file base_cmd cs_cmd cs_rb_env env_file
    local env_opts existing_container container_id container_mount container_name fs
    local total_cpu_partitions

    for this_cs_label in ${clients[@]} ${servers[@]}; do
        echo "working on osruntime for $this_cs_label"

        # Use this ssh before adding root user is permitted to use ssh
        this_cs_ssh_centos="ssh $ssh_opts -i $vm_ssh_key_file centos@${ips[$this_cs_label]} sudo"

        # Use this after root user is permitted to use ssh
        this_cs_ssh="ssh $ssh_opts -i $vm_ssh_key_file root@${ips[$this_cs_label]} "

        # enable ssh for root on VM
        echo "enabling root ssh for $this_cs_label"
        do_ssh $user@$host "$this_cs_ssh_centos echo \"PermitRootLogin yes\" >/etc/ssh/sshd_config"
        do_ssh $user@$host "$this_cs_ssh_centos cp /home/centos/.ssh/authorized_keys /root/.ssh/"
        do_ssh $user@$host "$this_cs_ssh_centos systemctl restart sshd"

        # create working directories
        do_ssh $user@$host "$this_cs_ssh /bin/mkdir -p $remote_cfg_dir"
        do_ssh $user@$host "$this_cs_ssh /bin/mkdir -p $remote_logs_dir"
        do_ssh $user@$host "$this_cs_ssh /bin/mkdir -p $remote_data_dir"
        echo "ensuring container image is pulled to $this_cs_label"
        do_ssh $user@$host "$this_cs_ssh podman pull $image"
        echo "Recording container image usage"
        do_ssh $user@$host "$this_cs_ssh echo $image $(date -u +%s) >>${remote_base_dir}/remotehost-container-image-census"
    done

    echo "$image" >$endpoint_run_dir/chroot-container-image.txt

    total_cpu_partitions=0
    for this_cs_label in ${clients[@]} ${servers[@]}; do
        set +u
        cpu_partitioning=0
        if [ ! -z "${cpuPartitioning[$this_cs_label]}" ]; then
            cpu_partitioning=${cpuPartitioning[$this_cs_label]}
        elif [ ! -z "${cpuPartitioning[default]}" ]; then
            cpu_partitioning=${cpuPartitioning[default]}
        fi
        set -u

        if [ "${cpu_partitioning}" == "1" ]; then
            (( total_cpu_partitions++ ))
        fi
    done

    # make tempdir on controller/undercloud
    do_ssh ${user}@${host} mkdir -p /var/lib/crucible/tmp

    # copy ssh key to controller/undercloud
    pushd "${config_dir}" >/dev/null
    do_scp "" rickshaw_id.rsa "${user}@${host}" /var/lib/crucible/tmp/
    popd >/dev/null

    # For each client and server launch the actual script which will run it.
    count=1
    for this_cs_label in ${clients[@]} ${servers[@]} ${collectors[@]}; do
        this_cs_ssh="ssh $ssh_opts -i $vm_ssh_key_file root@${ips[$this_cs_label]}"
        this_cs_scp="scp $ssh_opts -i $vm_ssh_key_file"

        container_name="${endpoint_label}_${run_id}_${this_cs_label}_${osruntime}"
        existing_container=`do_ssh $user@$host "$this_cs_ssh podman ps --all --format \"{{.Names}}\" | grep ^$container_name$"`
        if [ ! -z "$existing_container" ]; then
            echo "WARNING: found existing container '$existing_container', deleting"
            do_ssh $user@$host "$this_cs_ssh podman rm $container_name"
        fi
        this_cs_log_file="$this_cs_label.txt"

        set +u
        cpu_partitioning=0
        if [ ! -z "${cpuPartitioning[$this_cs_label]}" ]; then
            cpu_partitioning=${cpuPartitioning[$this_cs_label]}
        elif [ ! -z "${cpuPartitioning[default]}" ]; then
            cpu_partitioning=${cpuPartitioning[default]}
        fi
        set -u

        numa_node=-1
        this_disable_tools="$disable_tools"

        if [ "$osruntime" == "chroot" ]; then
            echo "using chroot"

            echo "Adding mount for chroot osruntime"
            container_id=$(do_ssh ${user}@${host} $this_cs_ssh podman create --name ${container_name} ${image})
            echo "container_id: [${container_id}]"
            echo "${this_cs_label} ${container_id}" >>${endpoint_run_dir}/chroot-container-id.txt
            container_mount=$(do_ssh ${user}@${host} $this_cs_ssh podman mount "${container_id}")
            echo "container_mount: ${container_mount}"
            echo "${this_cs_label} ${container_mount}" >>${endpoint_run_dir}/chroot-container-mount.txt
            echo "container_name: $container_name"
            echo "${this_cs_label} ${container_name}" >>${endpoint_run_dir}/chroot-container-name.txt
            if [ ! -z "${container_mount}" ]; then
                echo "Container mount: ${container_mount}"
                # Allow the user to more easily inspect failed runs
                echo "Mapping container /tmp to host ${remote_data_dir}"
                do_ssh ${user}@${host} $this_cs_ssh mkdir -p ${container_mount}/tmp
                do_ssh ${user}@${host} $this_cs_ssh mount --verbose --options bind ${remote_data_dir} ${container_mount}/tmp
                echo "Adding host directories to container mount"
                if [ "${host_mounts}" != "" ]; then
                    local oldIFS=${IFS}
                    IFS=" "
                    for fs in ${host_mounts}; do
                        chroot_rbind_mounts+=" ${fs}"
                    done
                    IFS=${oldIFS}
                fi
                for fs in ${chroot_rbind_mounts}; do
                    echo "Adding ${fs} to ${container_mount}"
                    do_ssh ${user}@${host} $this_cs_ssh mkdir -p ${container_mount}/${fs}
                    do_ssh ${user}@${host} $this_cs_ssh mount --verbose --options rbind /${fs} ${container_mount}/${fs}
                    do_ssh ${user}@${host} $this_cs_ssh mount --verbose --make-rslave ${container_mount}/${fs}
                    echo
                done
                echo "mounts:"
                do_ssh ${user}@${host} $this_cs_ssh mount | grep ${container_mount}
                # for chroot osruntime we can also simply copy the ssh key
                echo "Copying rickshaw ssh key to ${user}@${host}:${container_mount}/tmp/ for chroot runtime"
                do_ssh ${user}@${host} $this_cs_scp /var/lib/crucible/tmp/rickshaw_id.rsa ${ips[$this_cs_label]}:"${container_mount}/tmp/"
                do_ssh ${user}@${host} $this_cs_ssh /bin/cp /etc/hosts "${container_mount}/etc/"
                do_ssh ${user}@${host} $this_cs_ssh /bin/cp /etc/resolv.conf "${container_mount}/etc/"
            else
                echo "Container mount not found, exiting"
                exit 1
            fi

            base_cmd="/usr/local/bin/bootstrap"
            base_cmd+=" --rickshaw-host=$controller_ipaddr"
            base_cmd+=" --endpoint-run-dir=$endpoint_run_dir"
            base_cmd+=" $cs_rb_opts"
            base_cmd+=" --cs-label=$this_cs_label"
            base_cmd+=" --base-run-dir=$base_run_dir"
            base_cmd+=" --endpoint=remotehost"
            base_cmd+=" --osruntime=$osruntime"
            base_cmd+=" --max-sample-failures=$max_sample_failures"
            base_cmd+=" --max-rb-attempts=$max_rb_attempts"
            base_cmd+=" --cpu-partitions=${total_cpu_partitions}"
            base_cmd+=" --cpu-partition-index=${count}"
            base_cmd+=" --cpu-partitioning=$cpu_partitioning"
            base_cmd+=" --engine-script-start-timeout=$engine_script_start_timeout"
            base_cmd+=" --disable-tools=$this_disable_tools"
            if [ $numa_node -gt -1 ]; then
                base_cmd="numactl -N $numa_node -m $numa_node $base_cmd"
            fi

            # Note that --endpoint-run value must be hard-coded to /endpoint-run becaue of chroot
            # Same will be true for running podman
            cs_cmd="nohup chroot $container_mount $base_cmd \>$remote_logs_dir/$this_cs_log_file"
        elif [ "$osruntime" == "podman" ]; then
            echo "using podman"

            env_file="${this_cs_label}_env.txt"
            ssh_id=$(sed -z 's/\n/\\n/g' ${config_dir}/rickshaw_id.rsa)

            echo "rickshaw_host=$controller_ipaddr"         >> ${endpoint_run_dir}/${env_file}
            echo "endpoint_run_dir=$endpoint_run_dir"       >> ${endpoint_run_dir}/${env_file}
            echo "cs_label=$this_cs_label"                  >> ${endpoint_run_dir}/${env_file}
            echo "base_run_dir=$base_run_dir"               >> ${endpoint_run_dir}/${env_file}
            echo "cpu_partitioning=$cpu_partitioning"       >> ${endpoint_run_dir}/${env_file}
            echo "cpu_partitions=${total_cpu_partitions}"   >> ${endpoint_run_dir}/${env_file}
            echo "cpu_partition_index=${count}"             >> ${endpoint_run_dir}/${env_file}
            echo "endpoint=remotehost"                      >> ${endpoint_run_dir}/${env_file}
            echo "osruntime=$osruntime"                     >> ${endpoint_run_dir}/${env_file}
            echo "max_sample_failures=$max_sample_failures" >> ${endpoint_run_dir}/${env_file}
            echo "max_rb_attempts=$max_rb_attempts"         >> ${endpoint_run_dir}/${env_file}
            echo "ssh_id=${ssh_id}"                         >> ${endpoint_run_dir}/${env_file}
            echo "disable_tools=$this_disable_tools"        >> ${endpoint_run_dir}/${env_file}

            for cs_rb_opt in $cs_rb_opts; do
                arg=$(echo $cs_rb_opt | awk -F'=' '{print $1}')
                value=$(echo $cs_rb_opt | awk -F'=' '{print $2}')

                arg=$(echo ${arg} | sed -e 's/^--//' -e 's/-/_/g' )

                echo "${arg}=${value}"                      >> ${endpoint_run_dir}/${env_file}
            done

            if pushd ${endpoint_run_dir} >/dev/null; then
                do_ssh $user@$host mkdir -p $remote_cfg_dir 
                echo "Copying ${endpoint_run_dir}/${env_file} to ${user}@${host}:${remote_cfg_dir}"
                do_scp "" "${env_file}" "${user}@${host}" "${remote_cfg_dir}"
                echo "Copying from ${user}@${host}:${remote_cfg_dir} to ${ips[$this_cs_label]}:${remote_cfg_dir}"
                do_ssh $user@$host $this_cs_ssh mkdir -p $remote_cfg_dir 
                do_ssh $user@$host $this_cs_scp ${remote_cfg_dir}/${env_file} ${ips[$this_cs_label]}:${remote_cfg_dir} 
                popd >/dev/null
            else
                echo "Failed to pushd to ${endpoint_run_dir} to scp env file"
                exit 1
            fi

            cs_cmd="podman run"
            cs_cmd+=" --detach=true"
            cs_cmd+=" --name=${container_name}"
            cs_cmd+=" --env-file ${remote_cfg_dir}/${env_file}"
            cs_cmd+=" --privileged"
            cs_cmd+=" --ipc=host"
            cs_cmd+=" --pid=host"
            cs_cmd+=" --net=host"
            cs_cmd+=" --security-opt=label=disable"
            cs_cmd+=" --mount=type=bind,source=${remote_data_dir},destination=/tmp"
            cs_cmd+=" --mount=type=bind,source=/lib/modules,destination=/lib/modules"
            cs_cmd+=" --mount=type=bind,source=/usr/src,destination=/usr/src"
            if [ "$host_mounts" != "" ]; then
                local oldIFS=$IFS
                IFS=" "
                for fs in $host_mounts; do
                   cs_cmd+=" --mount=type=bind,source=$fs,destination=$fs"
                done
                IFS=$oldIFS
            fi
            cs_cmd+=" ${image}"
        fi

        echo -e "About to run:\n${cs_cmd}\n"
        if [ "$osruntime" == "chroot" ]; then
            do_ssh $user@$host $this_cs_ssh "${cs_cmd}" &
        elif [ "$osruntime" == "podman" ]; then
            do_ssh $user@$host $this_cs_ssh "${cs_cmd}"
        fi
        ssh_rc=$?
        if [ ${ssh_rc} -gt 0 ]; then
            echo "running ${osruntime} failed"
            exit 1
        fi

        let count=$count+1
    done
    echo "This endpoint deployed"
    echo
}

function cleanup_osruntime() {
    local this_cs_label vm_name mgmt_port_name

    for this_cs_label in ${clients[@]} ${servers[@]}; do
        vm_name="rickshaw-$run_id-$this_cs_label"
        mgmt_port_name="mgmt-$vm_name"
        echo "deletintg VM [$vm_name]"
        do_ssh ${user}@${host} "$osp_pre_cmd openstack server delete $vm_name" >${endpoint_run_dir}/openstack-server-delete-$vm_name.txt
        echo "deletintg network port [$mgmt_port_name]"
        do_ssh ${user}@${host} "$osp_pre_cmd openstack port delete $mgmt_port_name" >${endpoint_run_dir}/openstack-port-delete-$mgmt_port_name.txt
    done
    echo "deletintg keypair [$vm_ssh_key_name]"
    do_ssh ${user}@${host} "$osp_pre_cmd openstack keypair delete $vm_ssh_key_name" >${endpoint_run_dir}/openstack-keypair-delete-$vm_ssh_key_name.txt
    echo "deletintg keyfile [$vm_ssh_key_name]"
    do_ssh ${user}@${host} "rm -f $vm_ssh_key_file"
    do_ssh ${user}@${host} "$osp_pre_cmd openstack server list" >${endpoint_run_dir}/openstack-server-remaining.txt
    echo "remaining rickshaw openstack servers:"
    grep rickshaw ${endpoint_run_dir}/openstack-server-remaining.txt
    do_ssh ${user}@${host} "$osp_pre_cmd openstack port list" >${endpoint_run_dir}/openstack-port-remaining.txt
    echo "remaining rickshaw openstack ports:"
    grep rickshaw ${endpoint_run_dir}/openstack-port-remaining.txt
}


function move_osp_logs() {
    echo "moving engine logs"
    local this_cs_label remote_cs_log_file container_name delete_remote_dir
    mkdir -p $engine_logs_dir
    for this_cs_label in ${clients[@]} ${servers[@]} ${collectors[@]}; do
        this_cs_ssh="ssh $ssh_opts -i $vm_ssh_key_file root@${ips[$this_cs_label]}"
        remote_cs_log_file="$remote_logs_dir/$this_cs_label.txt"
        local_cs_log_file="$engine_logs_dir/$this_cs_label.txt"
        echo "this_cs_label: [$this_cs_label]"
        echo "remote_cs_log_file: [$remote_cs_log_file]"
        echo "this_cs_ssh: [$this_cs_ssh]"
        if [ "$osruntime" == "chroot" ]; then
            do_ssh $user@$host $this_cs_ssh cat $remote_cs_log_file >$local_cs_log_file
        elif [ "$osruntime" == "podman" ]; then
            container_name="${endpoint_label}_${run_id}_${this_cs_label}_${osruntime}"
            do_ssh $user@$host $this_cs_ssh podman logs ${container_name} >$local_cs_log_file
        fi
    done
}

function get_osp_config() {
    echo "getting OSP config"
    #TODO: get instances, networks, etc.
}

function endpoint_osp_sysinfo() {
    local remote_base_dir remote_dir local_dir
    remote_base_dir="/var/lib/crucible"
    remote_dir="${remote_base_dir}/${endpoint_label}_${run_id}"
    local_dir="${endpoint_run_dir}/sysinfo"

    mkdir ${local_dir}

    #TODO: get cluster version info, sosreport, etc
}

function endpoint_osp_cleanup() {
    move_osp_logs
    cleanup_osruntime
}

process_opts "$@"
process_osp_opts "$endpoint_opts"
init_common_dirs
load_settings

remote_base_dir="/var/lib/crucible"
remote_dir="${remote_base_dir}/${endpoint_label}_${run_id}"
remote_cfg_dir="${remote_dir}/cfg"
remote_logs_dir="${remote_dir}/logs"
remote_data_dir="${remote_dir}/data/tmp"
vm_ssh_key_dir=".rickshaw"
vm_ssh_key_name="$run_id-ssh-key"
vm_ssh_key_file="$vm_ssh_key_dir/$vm_ssh_key_name.pem"
osp_pre_cmd=". ./overcloudrc && "

osp_req_check
base_req_check
get_osp_config

echo "This endpoint to run these clients: ${clients[@]}"
echo "This endpoint to run these servers: ${servers[@]}"

# All roadblock particpants are not determined until it is known
# where tools are run.  Once this is known, this information needs
# to be sent back to the controller.
new_osp_followers=""
cs_servers=""
create_servers cs_servers cs ${clients[@]} ${servers[@]}
echo "These openstack servers were created: $cs_servers"
all_engines="$cs_servers"
launch_osruntime
# TODO: get active compute nodes
active_comput_nodes=""

if [ "${computes_tool_collect}" == "1" ]; then
    echo "Working on creating compute-tool engines"
    create_remotehost_engines compute_tool_engines compute $active_compute_nodes
    new_osp_followers+=" ${compute_tool_engines}"
    all_engines+=" $compute_tool_engines"
fi

controller_nodes=`cat "$endpoint_run_dir/controller-nodes.txt"`
echo "These nodes are controller: $controller_nodes"
if [ -z "${controller_nodes}" ]; then
    controllers_tool_collect="0"
fi
if [ "${controllers_tool_collect}" == "1" ]; then
    echo "Working on creating these controller-tool engines"
    create_remotehost_engines controller_tool_engines controller $controller_nodes
    new_osp_followers+=" ${controller_tool_engines}"
    all_engines+=" $controller_tool_engines"
fi

process_roadblocks osp ${new_osp_followers}


