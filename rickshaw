#!/usr/bin/perl
# -*- mode: perl; indent-tabs-mode: t; perl-indent-level: 4 -*-
# vim: autoindent tabstop=4 shiftwidth=4 expandtab softtabstop=4 filetype=perl
#
# Author: Andrew Theurer
#
# Rickshaw will run a benhcmark for you.  Please see README.md for instructions.

use strict;
use warnings;
use Cwd;
use Data::UUID;
use File::pushd;
use File::Temp qw(tempdir);
use JSON::XS;
use Data::Dumper;

my $ug = Data::UUID->new;
my %defaults = ( "num-samples" => 1, "tool-group" => "default", "test-order" => "i-s",
                 "run-dir" => tempdir(), "run-id" => $ug->create_str());

my $debug = 1;
my $use_roadblock = 0;
my %bench_config;
my @params;
my @endpoints;
my %run; # A multi-dimensional, nested hash, schema TBD
         # This hash documents what was run.
         # CDM utilities will convert this into many CDM documents.
print "\n";

sub usage {
    print "\nusage:\n\n";
    print "--bench-dir     Directory where benchmark helper project exists\n";
    print "--bench-params  File whith benchmark parameters to use\n";
    print "--num-samples   The number of sample exeuctions to run for each benchmark iteration\n";
    print "--test-order    's' = run all samples of an iteration first\n";
    print "                'i' = run all iterations of a sample first\n\n";
}

sub debug_log {
    if ($debug) {
        print "[DEBUG]" . shift;
    }
}

sub dump_params {
    my $params_ref = shift;
    my $params_str = "";
    foreach my $arg (keys %{ $params_ref }) {
        $params_str .= " --" . $arg . "=" . $$params_ref{$arg};
    }
    $params_str =~ s/^\s//;
    return $params_str;
}

sub put_json_file {
    my $filename = shift;
    my $json_ref = shift;
    my $coder = JSON::XS->new;
    debug_log(sprintf "trying to write [%s]\n", $filename);
    my $json_text = $coder->encode($json_ref);
    open(JSON_FH, ">" . $filename) || die("Could not open file $filename\n");
    printf JSON_FH "%s", $json_text;
    close JSON_FH;
}

sub get_json_file {
    my $filename = shift;
    my $coder = JSON::XS->new;
    debug_log(sprintf "trying to open [%s]\n", $filename);
    open(JSON_FH, $filename) || die("Could not open file $filename\n");
    my $json_text = "";
    while ( <JSON_FH> ) {
        $json_text .= $_;
    }
    close JSON_FH;
    my $perl_scalar = $coder->decode($json_text) || die "Could not read JSON";;
    return $perl_scalar;
}

foreach my $e (qw(RS_USER RS_EMAIL RS_TAGS RS_DESC)) {
    if (exists $ENV{$e}) {
        my $var = ($e =~ s/^RS_//);
        $run{$var} = $ENV{$e};
    }
}

# Process the cmdline params
while (scalar @ARGV > 0) {
    my $p = shift @ARGV;
    debug_log(sprintf "processing \@ARGV, param: [%s]\n", $p);
    my $arg;
    my $val;

    if ( $p =~ /^\-\-(\S+)/ ) {
        $arg = $1;
        if ( $arg =~ /^(\S+)=(.*)/ ) { # '--arg=val'
            $arg = $1;
            $val = $2;
        } else { # '--arg val'
            $val = shift @ARGV;
        }
    } else {
        print "[ERROR]malformed cmdline parameter: %s\n";
        usage;
        exit 1;
    }
    debug_log(sprintf "processing \@ARGV, arg is: [%s], val is: [%s]\n", $arg, $val);
    if ($arg eq "endpoint") {
        push(@endpoints, $val);
    } elsif ($arg =~ /^help$/) {
        usage;
        exit 0;
    } elsif ($arg =~ /^run-id$|^run-dir$|^bench-dir$|^bench-params$|^test-order$/ or
             $arg =~ /^tool-group$|^num-samples$|^name$|^email$|^tags$|^desc$/) {
        debug_log(sprintf "argument: [%s]\n", $arg);
        $run{$arg} = $val;
    } else {
        printf "[ERROR]argument not valid: [%s]\n", $arg;
        usage;
        exit 1;
    }
}

# Load the bench config and user params
exists $run{'bench-dir'} || die "[ERROR]You must use --bench-dir=/path/to/benchmark-subproject\n";
my $bench_config_file = $run{'bench-dir'} . "/config.json";
if (-e $bench_config_file) {
    my $bench_config_ref = get_json_file($bench_config_file);
    %bench_config = %{ $bench_config_ref };
    if (exists $bench_config{'benchmark'}) {
        printf "Preparing to run %s\n", $bench_config{'benchmark'};
        $run{'benchmark'} = $bench_config{'benchmark'};
    } else {
        print "[ERROR]benchmark was not defined in %s\n", $bench_config_file;
        exit 1;
    }
} else {
    printf "[ERROR]benchmark subproject config file %s was not found\n", $bench_config_file;
    exit 1;
}
exists $run{'bench-params'} || die "[ERROR}You must use " .
                                   "--bench-params=/path/to/benchmark-params.json\n";
@params = @{ my $params_ref = get_json_file($run{'bench-params'}) };

# Apply defaults
foreach my $p (keys %defaults) {
    if (! exists $run{$p}) {
        debug_log(sprintf "applying default value [%s] for %s\n", $defaults{$p}, $p);
        $run{$p} = $defaults{$p};
    }
}

# Ensure the bench-dir and run-dir have absolute paths
for my $dirtype (qw(run-dir bench-dir)) {
    {
        my $dir = pushd($run{$dirtype});
        debug_log(sprintf "pushd to [%s]\n", $run{$dirtype});
        my $cwd = getcwd();
        debug_log(sprintf "cwd [%s]\n", $cwd);
        $run{$dirtype} = $cwd;
    }
}
-e $run{'run-dir'} || mkdir($run{'run-dir'});
printf "Run directory: [%s]\n", $run{'run-dir'};
printf "Bench directory: [%s]\n", $run{'bench-dir'};

# If there are no endpoints, assume 1 endpoint using the 'local' extension
if (scalar @endpoints == 0) {
    if (exists $bench_config{'client'} and exists $bench_config{'server'}) {
        push(@endpoints, "local:client[1],server[1]");
    } else {
        push(@endpoints, "local:client[1]");
    }
}

# Call each endpoint script with "--validate" as the first option, and each endpoint script should
# return a list of clients and servers which are used from this endpoint.  Collect this output
# and verify there are no gaps in the numbering of clients, and if the benchmark uses servers,
# that there is 1 server for every client.
# Why can't we just parse the endpoint option?  Because there is no gaurantee that the endpoint
# option always contains the client and server IDs that will be used.  For example, an endpoint
# for k8s might look like: "--endpoint:[1-5]" where client and server are not required, and when
# not used, this endpoint assumes both clients and servers (for IDs 1-5) will be deployed. 
# This ensures the format of the specific endpoint option string is completely up to that endpoint
# and not rickshaw.
my %clients_servers;
my $min_id;
my $max_id;
printf "Confirming the endpoints will satisfy the benchmark-client ";
printf "and benchmark-server " if exists $bench_config{'server'};
printf "requirements\n";
foreach my $endpoint (@endpoints) {
    if ($endpoint =~ /(^\w+):(.+$)/) {
        my $type = $1;
        my $opts = $2;
        my $dir = pushd("./endpoints/" . $type);
        my $cmd = "./" . $type . " --validate " . $opts;
        my @output = `$cmd`;
        # Output from endpoint's validation should be 1 or more lines with "client" or "server"
        # followed by 1 or more positive integers representing the client/server IDs this
        # endpoint handles::
        # client <int> [int]
        # server <int> [int]
        foreach my $line (@output) {
            chomp $line;
            if ($line =~ /(client|server)\s+(.+)$/) {
                my $t = $1;
                my $ids = $2;
                foreach my $id (split(/\s+/, $ids)) {
                    my %info = ( 'endpoint-type' => $type, 'id' => $id );
                    $clients_servers{$t}[$id - 1] = \%info;
                    $min_id = $id if (! defined $min_id or $id < $min_id);
                    $max_id = $id if (! defined $max_id or $id > $max_id);
                }
            } else {
                printf "[ERROR]output from endpoint validation incorrect:\n%s\n", $line;
                exit 1;
            }
        }
    } else {
        printf "[ERROR]endpoint value is not valid: [%s]\n", $endpoint;
        exit 1;
    }
}
if ($min_id != 1) {
    printf "[ERROR]lowest ID found in clients and servers is %d, must be 1\n", $min_id;
    exit 1;
}
for (my $id = $min_id; $id <= $max_id; $id++) {
    debug_log(sprintf "checking for client ID %d\n", $id);
    if (! defined $clients_servers{'client'}[$id - 1]) {
        printf "[ERROR]client ID %d is not defined in ID range %d - %d\n", $id, $min_id, $max_id;
        exit 1;
    }
    if (exists $bench_config{'server'}) {
        debug_log(sprintf "checking for server ID %d\n", $id);
        if (! defined $clients_servers{'server'}[$id - 1]) {
            printf "[ERROR]server ID %d is not defined in ID range %d - %d\n",
                   $id, $min_id, $max_id;
            exit 1;
        }
    } else {
        debug_log(sprintf "checking for NO server ID %d\n", $id);
        if (defined $clients_servers{'server'}[$id - 1]) {
            printf "[ERROR]server ID %d is defined in ID range %d - %d, but this benchmark " .
                   "does not use servers\n", $id, $min_id, $max_id;
            exit 1;
        }
    }
}
printf "There will be %d clients", $max_id;
printf " and servers" if exists $bench_config{'server'};
printf "\n";

$run{'endpoints'} = \@endpoints;

# Build test execution order
my @tests;
if ($run{'test-order'} eq 's') {
    for (my $iid = 0; $iid < scalar @params; $iid++) {
        for (my $sid = 0; $sid < $run{'num-samples'}; $sid++) {
            my %test = ('iter_id' => $iid, 'samp_id' => $sid);
            push(@tests, \%test);
        }
    }
} elsif ($run{'test-order'} eq 'i') {
    for (my $sid = 0; $sid < $run{'num-samples'}; $sid++) {
        for (my $iid = 0; $iid < scalar @params; $iid++) {
            my %test = ('iter_id' => $iid, 'samp_id' => $sid);
            push(@tests, \%test);
        }
    }
} else {
    printf "[ERROR]Value for --test-order [%s] is not valid\n", $run{'test-order'};
    usage;
    exit 1;
}

# Run on the controller (the host running this script) the benchmark-specific "pre-script"
my $endpoint_scripts_dir = $run{'run-dir'} . "/endpoint-scripts";
if (exists $bench_config{"controller"}{"pre-script"} and $bench_config{"controller"}{"pre-script"} ne "") {
    my $dir = pushd($run{'run-dir'});
    # Note that the user params for the benchmark are from the first set only
    # This pre-script is run only once before all of the tests are started.
    # If this script generates a file to aid in benchmark execution (such as a job file),
    # The file should work for all perams sets (all benchmark iterations).  If you need
    # different job files per iteration, then use the client or server-side "pre-script"
    # to either augment a file generated here or create a completely new file.
    my $cmd = $bench_config{"controller"}{"pre-script"} . " " . dump_params($params[0]);
    $cmd =~ s/\%bench-dir\%/$run{'bench-dir'}/g;
    $cmd =~ s/\%run-dir\%/$run{'run-dir'}/g;
    debug_log(sprintf "controller pre-script command: [%s]\n", $cmd);
    my $pre_cmd_output = `$cmd . '2>&1'`;
    debug_log(sprintf "controller pre-script output:\n%s\n", $pre_cmd_output);
}

# Build the client and server scripts
# TODO: include roadblock commands
# TODO: run all tests, not just the first one (requires roadblock for synchronization)
foreach my $cs_type (keys %clients_servers) {
    foreach my $cs_ref (@{ $clients_servers{$cs_type} }) {
        printf "type: [%s]\n", $cs_type;
        my $id = $$cs_ref{'id'};
        my $param_id = $id - 1;
        foreach my $test_ref (@tests) {
            my $test_iter = $$test_ref{'iter-id'};
            my $test_samp = $$test_ref{'samp-id'};
            # For each client/server and for each test, we need:
            # -pre-benchmark-execution command
            # -roadblock sync to begin this test (not yet implmented here)
            # -apply regex to the benchmark command and run command
            # -roadblock sync to end this test (not yet implmented here)
            # -post-benchmark-execution command
            -e $endpoint_scripts_dir || mkdir($endpoint_scripts_dir) ||
                die "[ERROR]Could not create directory for endpoint" .
                    " scripts: [" . $endpoint_scripts_dir . "]\n";
        
            my $script_file = $endpoint_scripts_dir . "/" . $cs_type . "-" . $id;
            open(FH, ">" . $script_file) || die "[ERROR]could not open script file for writing: ["
                                                . $script_file . "]\n";
            debug_log(sprintf "writing script file [%s]\n", $script_file);
            print FH "#!/usr/bin/env bash\n";
            print FH "exec 1>" . $cs_type . "-" . $id . "-stdout.txt 2>" . $cs_type . "-" . $id . "-stderr.txt\n";
            print FH "echo Hello\n";
            if (exists $bench_config{$cs_type}{"pre-script"} and
            $bench_config{$cs_type}{"pre-script"} ne "") {
                printf FH "%s\n", $bench_config{$cs_type}{"pre-script"};
            }
            if (exists $bench_config{$cs_type}{"bin"} and
            $bench_config{$cs_type}{"bin"} ne "") {
                die "[ERROR][BUG]param missing\n" if ! defined $params[$param_id];
                my $cmd = $bench_config{$cs_type}{"bin"} . " " . dump_params($params[$param_id]);
                debug_log(sprintf "cmd: [%s]\n", $cmd);
                # Apply a regex from the benchmark config file to the command
                    # This is used to remove things like "--clients=" because the
                # native benchmark does not understand this parameter
                if ($bench_config{$cs_type} and $bench_config{$cs_type}{"param_regex"}) {
                    for my $r (@{ $bench_config{$cs_type}{"param_regex"} }) {
                        # to apply the 's/x/y/' regex from the file, some eval trickery is necessary
                        # todo: first test the $r regex separately for [perl syntax] errors with eval
                        # before doing below
                        $cmd = eval "\$_='$cmd'; $r; return scalar \$_";
                    }
                }
                printf FH "%s\n", $cmd;
            } else {
                die "[ERROR]Could not find client binary in bench_config\n";
            }
            close FH;
            chmod 0755, $script_file;
            last; # for now only do the first test since we don't have roadblock integrated
        }
    }
}

# Deploy ths endpoints so they are ready to run benchmark and tools
printf "\nendpoint output:\n";
foreach my $endpoint (@endpoints) {
    (my $type, my $endpoint_opts) = split(/:/, $endpoint);
    if (-e "./endpoints/" . $type) {
        my $dir = pushd("./endpoints/" . $type);
        my $cmd = "./" . $type . " " . $endpoint_opts . " " . $run{'run-dir'} . " " . $endpoint_scripts_dir;
        debug_log(sprintf "going to run %s\n", $cmd);
        # The below 'system' needs to be forked, then wait for all to finish.
        # The endpoint program should get all clients/servers "ready", that is,
        # waiting for instructions from roadblock.  The above command needs
        # info about how to contact roadblock.
        # Endpoints should return for each client and server started:
        # - the ID of the client/server
        # - the roadblock client ID
        my $rc = system ($cmd);
        debug_log(sprintf "exit code from endpoint: %d\n", $rc);
    } else {
        printf "[ERROR]could not find endpoint ./endpoints/%s\n", $type;
        exit 1;
    }
}

# Build the benchmark command for the clients and servers for each iteration
####my %endpoint_cmds;
####for (my $iid = 0; $iid < scalar @params; $iid++) {
####debug_log(sprintf "building %s number %d pre-benchmark command for iteration %d\n",
####"client", 1, $iid);
####}

# Cycle through the list of tests using roadblock to kick them off
my @iterations;
# printf "\nRequesting endpoints to start tools\n";
# TODO: tell roadblock to request tool-start at endpoints
# TODO: wait for roadblock to get ack that tools have started
# printf "Tools have started on endpoints (not really)\n";
for (my $tid = 0; $tid < scalar @tests; $tid++) {
    # printf "\nRequesting endpoints to start test for iteration %d, sample %d\n",
    #        $tests[$tid]{'iter_id'}, $tests[$tid]{'samp_id'};
    # TODO: build per-client/server commands to run benchmark and send to roadblock
    # TODO: wait for roadbloack to get ack from endpoints that they have started the test
    # printf "Test has started\n";
    # TODO: wait for roadblock to get ack that test is complete (also have timeout)
    # printf "Test has completed\n";
    $iterations[$tests[$tid]{'iter_id'}]{'params'} = \%{ $params[$tests[$tid]{'iter_id'}] };
}
# printf "\nRequesting endpoints to stop tools\n";
# TODO: tell roadblock to request tool-stop at endpoints
# TODO: wait for roadblock to get ack that tools have stopped
# printf "Tools have stopped on endpoints\n";
# printf "Requesting endpoints to send test results\n";
# TODO: tell roadblock to request test data
# printf "Test results from endpoints received\n";

$run{'iterations'} = \@iterations;
put_json_file("rickshaw.json", \%run);
